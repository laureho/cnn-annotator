{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0xrtPBJm1p8"
   },
   "source": [
    "# CNN Training for Cell Cycle State Classification\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This notebook allows you to train a convolutional neural network (CNN) using your annotated single-cell image patches. \n",
    "Follow the step-wise instructions to proceed with the network training and evaluation. \n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "1. If using the virtual environment of [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb \"Google Colaboratory\"): You will need to be signed in with a Google email address. Your session will 'timeout' if you do not interact with it. Although documentation claims the runtime should last 90 minutes if you close the browser or 12 hours if you keep the browser open, our experience shows it should disconnect after 60 minutes even if you keep the browser open. Please visit this [StackOverflow](https://stackoverflow.com/questions/54057011/google-colab-session-timeout \"Google Colab Session Timeout\") discussion where users report different experiences with Colab's session timeout. Additionally, please remember your access to Colab resources is limited to a maximum of 12h per session. If you exceed this limit, your access to Colab may be temporarily suspended by Google.\n",
    "\n",
    "2. To be able to train the neural network on your own data, you must first **import your annotated data** into the folders to source from. Please follow the running instructions below.\n",
    "\n",
    "\n",
    "### Running Instructions:\n",
    "\n",
    "1. Execute the first cell containing code below. This will install our [CellX](https://github.com/quantumjot/cellx) library & create local directories in the environment of the virtual machine. The executed first cell will print ```Building wheel for cellx (setup.py) ... done```. \n",
    "\n",
    "2. Click on the ``` 📁``` folder icon located on the left-side dashboard of the Colab notebook. You should now see 4 subfolders in this directory: `sample_data` (default), `logs`, `train` and `validation` folder, which should all be empty. **Manually drag your 'annotation_XXX.zip' files into the `train` and `validation` folders**.\n",
    "\n",
    "3. Prior to training the model, the training & validation sets are created and image augmentations introduced to the training set. In the next step the model is trained based on the default parameters defined in the **\"Set up CNN training hyperparameters:\"** section, but you are welcome to modify the values if you wish.\n",
    "\n",
    "4. You can now run the entire notebook by clicking on ```Runtime``` > ```Run all``` in the upper main dashboard. Note: Re-running the initial cell will fail to create the `logs`, `train` and `validation` folders as they have already been created, but this won't prevent the subsequent cells from running.\n",
    "\n",
    "5. During training, you can actively visualise what the network is doing via [TensorBoard](https://www.tensorflow.org/tensorboard/get_started \"TensorFlow || Tensorboard\"), a tool for providing the measurements and visualisations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualising the model graph, projecting embeddings to a lower dimensional space, and much more.\n",
    "\n",
    "---\n",
    "\n",
    "**Happy training!**\n",
    "\n",
    "*Your [CellX](http://lowe.cs.ucl.ac.uk/cellx.html \"Lowe Lab @ UCL\") team*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q72wbtUm1qD"
   },
   "source": [
    "### Install the CellX library & create subdirectories in the virtual machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOtdYdHfm1qD"
   },
   "outputs": [],
   "source": [
    "# if using colab, install cellx library and make log and data folders\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    try: \n",
    "        import cellx\n",
    "    except:\n",
    "        !pip install -q git+git://github.com/quantumjot/cellx.git\n",
    "        !mkdir logs\n",
    "        !mkdir train\n",
    "        !mkdir validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxGN9I8Tm1qE"
   },
   "source": [
    "### Import libraries and CellX toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI0vgg9jm1qE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSQ2HRVJm1qE"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4FcQHy6m1qE"
   },
   "outputs": [],
   "source": [
    "from cellx.layers import Encoder2D\n",
    "from cellx.tools.dataset import build_dataset\n",
    "from cellx.tools.dataset import write_dataset\n",
    "from cellx.augmentation.utils import append_conditional_augmentation, augmentation_label_handler\n",
    "from cellx.callbacks import tensorboard_confusion_matrix_callback\n",
    "from cellx.tools.io import read_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc6DAoZ3m1qF"
   },
   "source": [
    "### Define paths & class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw1ipBL0m1qF"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./train\"\n",
    "VAL_PATH = \"./validation\"\n",
    "TRAIN_FILE = os.path.join(TRAIN_PATH, 'CNN_train.tfrecord')\n",
    "VAL_FILE = os.path.join(VAL_PATH, 'CNN_validation.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv5QAp3um1qF"
   },
   "source": [
    "### Set up CNN training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-QTgXl7m1qF"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20_000\n",
    "TRAINING_EPOCHS = 100\n",
    "BOUNDARY_AUGMENTATION = True\n",
    "INPUT_SHAPE = (64, 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JAxyQQxm1qF"
   },
   "source": [
    "### Load the TensorBoard extension for real-time visualisation of CNN training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PFrffAZm1qF"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "LOG_ROOT = './logs'\n",
    "LOG_DIR = os.path.join(LOG_ROOT, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVvw12DTm1qG"
   },
   "source": [
    "### Generate TensorFlow Record (TFRecord) files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHdOede2m1qG"
   },
   "outputs": [],
   "source": [
    "def create_tf_record(\n",
    "    root: str, \n",
    "    filename: str,\n",
    "    use_flagged: bool = False\n",
    "):\n",
    "\n",
    "    # load the annotations\n",
    "    _images, _labels, _states = read_annotations(root, use_flagged=use_flagged)\n",
    "    images_arr = np.stack(_images, axis=0)[..., np.newaxis]\n",
    "    labels_arr = np.stack(_labels, axis=0)\n",
    "    \n",
    "    # write the tf dataset\n",
    "    write_dataset(filename, images_arr.astype(np.uint8), labels=labels_arr.astype(np.int64))\n",
    "\n",
    "    # return the state labels \n",
    "    states = [k for k, v in sorted(_states.items(), key=lambda item: item[1])]\n",
    "\n",
    "    # plot some stats \n",
    "    stats = {k: _labels.count(v) for k, v in _states.items()}\n",
    "    print(f\"Exported \\'{filename}\\' containing:\")\n",
    "    if not use_flagged: print(f\" - Excluding flagged files\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\" - [{_states[k]}] {k}: {v}\")\n",
    "    print(f\" - Total images: {images_arr.shape[0]}\")\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzgrYn4Mm1qG"
   },
   "source": [
    "### IMPORTANT!\n",
    "\n",
    "**Prior to calling the function to create the TFRecord files:**\n",
    "\n",
    "You need to manually drag the annotation_XXX.zip files into the newly created folders (if you haven't yet done so from following the running instructions at the top of the notebook). If you are working in the Google Colab environment, click on the folder icon on the left-side dashboard, which should now contain the `logs`, `train` and `validation` directories. They should be empty until you drag your annotation files into them.\n",
    "\n",
    "Once the files have been imported, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gjsrmn4Gm1qG"
   },
   "outputs": [],
   "source": [
    "LABELS = create_tf_record(TRAIN_PATH, TRAIN_FILE, use_flagged=not BOUNDARY_AUGMENTATION)\n",
    "_ = create_tf_record(VAL_PATH, VAL_FILE, use_flagged=not BOUNDARY_AUGMENTATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrLhVmunm1qG"
   },
   "source": [
    "### Create a simple CNN for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGgzMUfmm1qG"
   },
   "outputs": [],
   "source": [
    "img = K.layers.Input(shape=INPUT_SHAPE)\n",
    "x = Encoder2D(layers=[8, 16, 32, 64, 128])(img)\n",
    "x = K.layers.Flatten()(x)\n",
    "x = K.layers.Dense(256, activation=\"relu\")(x)\n",
    "x = K.layers.Dropout(0.2)(x)\n",
    "logits = K.layers.Dense(len(LABELS), activation=\"linear\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECXUoYTMm1qG"
   },
   "outputs": [],
   "source": [
    "model = K.Model(inputs=img, outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO-zelEnm1qH"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqYuOJzmm1qH"
   },
   "source": [
    "### Set up some augmentations to be used while training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RolEOzdkm1qH"
   },
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def normalize(img):\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    # clip to 4 standard deviations\n",
    "    img = tf.clip_by_value(img, -4., 4.)\n",
    "    tf.debugging.check_numerics(img, \"Image contains NaN\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaAOKscam1qH"
   },
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def augment(img):\n",
    "    if BOUNDARY_AUGMENTATION:\n",
    "        # this will randomly simulate the cropping that occurs at the edge of\n",
    "        # an image volume\n",
    "\n",
    "        vignette = np.ones(INPUT_SHAPE, dtype=np.float32)\n",
    "        width = np.random.randint(0,INPUT_SHAPE[0]//2)\n",
    "        vignette[:,:width,...] = 0\n",
    "\n",
    "        img = tf.cond(pred=tf.random.uniform(shape=())<0.05,\n",
    "                true_fn=lambda: tf.multiply(img, vignette),\n",
    "                false_fn=lambda: img)\n",
    "\n",
    "    # do some data augmentation\n",
    "    k = tf.random.uniform(maxval=3, shape=(), dtype=tf.int32)\n",
    "    img = tf.image.rot90(img, k=k)\n",
    "\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eimItukpm1qH"
   },
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def random_contrast(x):\n",
    "    return tf.image.random_contrast(x, 0.3, 1.0)\n",
    "\n",
    "@augmentation_label_handler\n",
    "def random_brightness(x):\n",
    "    return tf.image.random_brightness(x, 0.3, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEG82mW7m1qI"
   },
   "source": [
    "### Build the training dataset, with random augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlTndbA_m1qI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = build_dataset(TRAIN_FILE, read_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoqJyaBym1qI"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(augment)\n",
    "dataset = append_conditional_augmentation(dataset, [random_contrast, random_brightness])\n",
    "dataset = dataset.map(normalize)\n",
    "dataset = dataset.shuffle(buffer_size=BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn1nSEzzm1qI"
   },
   "source": [
    "### Build the validation dataset, without augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECP5UPcQm1qI"
   },
   "outputs": [],
   "source": [
    "validation_dataset = build_dataset(VAL_FILE, read_label=True)\n",
    "validation_dataset = validation_dataset.map(normalize)\n",
    "validation_dataset = validation_dataset.take(-1).as_numpy_iterator()\n",
    "\n",
    "validation_images, validation_labels = zip(*list(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw1y23j8m1qI"
   },
   "source": [
    "### Set up TensorBoard callbacks to monitor training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90bpVbU6m1qI"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = K.callbacks.TensorBoard(log_dir=LOG_DIR)\n",
    "confusion_matrix_callback = tensorboard_confusion_matrix_callback(\n",
    "    model, \n",
    "    np.asarray(validation_images), \n",
    "    validation_labels,\n",
    "    LOG_DIR,\n",
    "    class_names=LABELS,\n",
    "    is_binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCsvx5Wym1qI"
   },
   "source": [
    "### Set up the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-RACBplm1qJ"
   },
   "outputs": [],
   "source": [
    "loss = K.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\", loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIh-cxg-m1qJ"
   },
   "source": [
    "## Finally, train the model and evaluate performance using TensorBoard:\n",
    "\n",
    "Running the next cell will show the TensorBoard GUI, which will show the calculations of the model performance after each epoch. \n",
    "You can monitor the model performance while progressing through training epochs by switching to the `SCALARS` (accuracy & loss) or `IMAGES` (confusion matrix) tabs in the upper menu bar. \n",
    "\n",
    "Note: If you only see the `GRAPHS` tab and at least one epoch has already completed, go to the dropdown on the right side (will probably show `INACTIVE`) and choose `SCALARS` from the list then press the refresh button to the right of the dropdown.\n",
    "Press the refresh button again at any later point to visualise the most up-to-date model performance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DswRNnP4m1qJ"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir $LOG_ROOT --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DfOllDim1qJ"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    dataset, \n",
    "    steps_per_epoch=BUFFER_SIZE//BATCH_SIZE, \n",
    "    epochs=TRAINING_EPOCHS, \n",
    "    callbacks=[tensorboard_callback, confusion_matrix_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXXpzausm1qJ"
   },
   "source": [
    "### Save the Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdrkVzYZm1qJ"
   },
   "source": [
    "**Do not terminate this notebook before saving the model.** \n",
    "To export the saved model to your local machine, press the '...' button next to the `model.h5` file on the left-side dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCQP5QeRm1qJ"
   },
   "outputs": [],
   "source": [
    "model_name = 'model'\n",
    "model.save('{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApWKYXRDm1qJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "C_CNN_Training_and_Validation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
