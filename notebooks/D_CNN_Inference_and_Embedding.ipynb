{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Inference for Cell Cycle State Classification\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This notebook allows you to take the convolutional neural network (CNN) that you trained in the previous notebook and use it for inference on previously unseen single-cell image patches. Follow the step-wise instructions to proceed with testing the network.\n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "1. You are using the virtual environment of the [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb \"Google Colaboratory\"). To be able to test the neural network, you must first **import images not used during training** into the folder to source from. Please follow the running instructions after executing the first cell of this notebook.\n",
    "\n",
    "2. If using Google Colab: You will need to be signed in with a Google email address. Your session will 'timeout' if you do not interact with it. Although documentation claims the runtime should last 90 minutes if you close the browser or 12 hours if you keep the browser open, our experience shows it should disconnect after 60 minutes even if you keep the browser open. Please visit this [StackOverflow](https://stackoverflow.com/questions/54057011/google-colab-session-timeout \"Google Colab Session Timeout\") discussion where others have reported even shorter periods of time until the runtime disconnects when failing to interact with the session. Additionally, please remember your access to Colab resources is limited to a maximum of 12h per session. If you exceed this limit, your access to Colab may be temporarily suspended by Google.\n",
    "\n",
    "\n",
    "### Running Instructions:\n",
    "\n",
    "1. Execute the first cell containing code below, which will install the CellX library & create a local test directory in the environment of the virtual machine. The executed first cell will print ```Building wheel for cellx (setup.py) ... done```. (Note: This virtual environment is different from the one created for the Training notebook, which is why we need to re-install the external `cellx` library etc.)\n",
    "\n",
    "2. Click on the ``` 📁``` folder icon located on the left-side dashboard of the Colab notebook, this is the default `content` directory where you can see the following subdirectories: `sample_data` (default) & `test`. Drag your saved model (the `.h5` file) into the `content` folder and your annotated zip file(s) into the `test` folder.\n",
    "\n",
    "3. You can now now run the entire notebook by clicking on ```Runtime``` > ```Run``` in the upper main dashboard. \n",
    "\n",
    "---\n",
    "\n",
    "**Happy testing!**\n",
    "\n",
    "*Your [CellX](http://lowe.cs.ucl.ac.uk/cellx.html \"Lowe Lab @ UCL\") team*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the CellX library & create subdirectories in the virtual machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using colab, install cellx library and make log and data folders\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install -q git+git://github.com/quantumjot/cellx.git\n",
    "    !mkdir test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and CellX toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.special import softmax\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cellx.core import load_model\n",
    "from cellx.layers import Encoder2D\n",
    "from cellx.tools.confusion import plot_confusion_matrix\n",
    "from cellx.tools.io import read_annotations\n",
    "from cellx.tools.projection import ManifoldProjection2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths & class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"./test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import test dataset from the zip files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels, states = read_annotations(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the \"load_model\" function from the CellX library, we can import models without needing to specify the CellX custom layers that had been used to build them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model'\n",
    "model = load_model(f'{model_name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the images in the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image normalization function\n",
    "def normalize_image_array(img):\n",
    "    img_mean = np.mean(img)\n",
    "    img_stddev = max(np.std(img), 1.0/np.size(img))\n",
    "    img = np.subtract(img,img_mean)\n",
    "    img = np.divide(img,img_stddev)\n",
    "    # clip to 4 standard deviations\n",
    "    img = np.clip(img, -4, 4)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [normalize_image_array(image) for image in test_images]\n",
    "test_images_array = np.array(test_images)[...,np.newaxis] # convert to numpy array for model prediction\n",
    "test_labels_array = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model on the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_images_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'softmax' function transforms test_predictions into an array of scores for each class for each instance in the testing set. Across classes, the scores sum to one. The class associated with the highest score is the model's 'prediction'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = softmax(test_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show predictions on the test images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample N images out of the testing set to check the model's predictions on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_testing_predictions(\n",
    "    num_examples, # number of testing examples to show\n",
    "    test_images\n",
    "):\n",
    "    plt.figure(figsize=(10,3*(int(num_examples/5)+1)))\n",
    "    plt.suptitle('Predictions',fontsize=25,x=0.5,y=0.95)\n",
    "    for image_num in range(min(np.shape(test_images_array)[0],num_examples)-1):\n",
    "        plt.subplot(int(num_examples/5)+1,5,image_num+1)\n",
    "        plt.imshow(test_images_array[image_num,:,:,0])\n",
    "        plt.title('Image {}'.format(image_num+1))\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "        plt.xlabel(list(states)[np.argmax(test_predictions[image_num])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_testing_predictions(20,test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate evaluation metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next calculate the \"precision\", \"recall\" and \"F1 score\" metrics for each class, as well as the \"confusion matrix\" for the CNN's performance on the testing set. The three metrics are calculated using the number of \"false positive\", \"true positive\" and \"false negative\" predictions for each class.\n",
    "- The \"precision\" of class X is calculated by $$precision(X) = \\frac{No.\\;of\\;true\\;positives}{No.\\;of\\;true\\;positives+No.\\;of\\;false\\;positives}$$\n",
    "- The \"recall\" of class X is calculated by $$recall(X) = \\frac{No.\\;of\\;true\\;positives}{No.\\;of\\;true\\;positives+No.\\;of\\;false\\;negatives}$$\n",
    "- The \"F1 score\" of class X is calculated by $$F1(X) = 2*\\frac{precision(X)*recall(X)}{precision(X)+recall(X)}$$\n",
    "<br>\n",
    "\n",
    "The \"confusion matrix\" is a table that visually represents the performance of a network on a testing set. The number shown in row A and column B is the number of testing examples of ground-truth class A that have been predicted as belonging to class B by the network.\n",
    "\n",
    "Reading resource for confusion matrices: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = model.evaluate(test_images_array, test_labels_array)\n",
    "\n",
    "test_confusion_matrix = confusion_matrix(test_labels,np.argmax(test_predictions,axis=1))\n",
    "test_confusion_matrix_plot = plot_confusion_matrix(test_confusion_matrix,list(states))\n",
    "test_confusion_matrix_plot.show()\n",
    "\n",
    "print('Testing Accuracy = ',accuracy)\n",
    "print('Testing Loss = ',loss)\n",
    "\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(test_labels,np.argmax(test_predictions,axis=1))\n",
    "print('Testing Precision = ',precision)\n",
    "print('Testing Recall = ',recall)\n",
    "print('Testing F1 Score = ',fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with UMAP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the below cell, we see that the model output is an array of 2 dimensions: \n",
    "* the 1st dimension corresponds to the number of test images used \n",
    "* the 2nd dimension corresponds to the number of possible classes pre-defined in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use UMAP to easily visualise the network's classification performance by embedding the predictions from 5D space (number of classes/features) into a lower 2D space while attempting to keep the data's inherent structure and underlying relationships.\n",
    "\n",
    "We first define our parameters of choice. In this simple example, we chose to only modify the following ones:\n",
    "* `n_neighbors` - the number of neighbours determines the size of the local neighbourhood that UMAP should focus on when creating the embedding, low values => emphasis on local structure, high values => emphasis on global structure\n",
    "* `n_epochs` - the number of epochs determines the number of rounds the UMAP embedding will be optimised for (similar to training a CNN), the higher the number the more accurately the 2D embedding will replicate the original data structure\n",
    "* `random_state` - UMAP is a stochastic algorithm, so we need to set a random seed to ensure that the results are reproducible across different runs. try eliminating this parameter, you should see slightly different UMAP embeddings from one run to the next\n",
    "\n",
    "Feel free to adjust the parameters and check how the below image projection changes! You can read up on the most important parameters [here](https://umap-learn.readthedocs.io/en/latest/parameters.html#) or go through the whole list of parameters [here](https://umap-learn.readthedocs.io/en/latest/api.html).\n",
    "\n",
    "If you're interested in reading about how UMAP works, [see here](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP parameters\n",
    "nbs = 5\n",
    "eps = 50\n",
    "rnd = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a UMAP model with the defined parameters. The full configuration of the UMAP model will be printed out with all the parameter values to be used, including the ones modified above. \n",
    "\n",
    "Note:`verbose=True`enables written feedback to the user while UMAP is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = UMAP(n_neighbors=nbs, n_epochs=eps, random_state=rnd, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the UMAP model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.fit(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D image patch projection of model embedding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By projecting the test images corresponding to the test predictions on top of the UMAP embedding, we can visually assess whether single-cell patches of the same class correctly cluster together in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert single-channel test images to rgb three-channel images\n",
    "print(f\"shape of test images: {test_images_array.shape}\")\n",
    "rgb_images = np.concatenate([test_images_array]*3, axis=-1)\n",
    "print(f\"shape of rgb test images: {rgb_images.shape}\")\n",
    "# normalise image values to 0-1 range (Min-Max scaling) & convert to 8-bit\n",
    "rgb_images = ((rgb_images-np.min(rgb_images))/(np.ptp(rgb_images)) * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the grid of image patches corresponding to the UMAP embedding. This is basically a 2D histogram where points on a same grid cell are binned and the average of the corresponding images is calculated before being overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = ManifoldProjection2D(rgb_images)\n",
    "img_grid, heatmap, delimiters = projection(mapper.embedding_, components=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a figure to show the image projection. \n",
    "You can uncomment the last line if you want to save the projection as `.png` file, it will appear in the Files tab (if you don't see it, press the middle Refresh button at the top of the tab). \n",
    "Remember to then go on the \"...\" button to the right of the `.png` file to download it.\n",
    "Reminder: Files saved during a Colab session will be lost upon closing this session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "im = plt.imshow(img_grid,\n",
    "                origin=\"lower\",\n",
    "#                 extent=delimiters, \n",
    "                cmap=\"gray\",)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.colorbar()\n",
    "\n",
    "# (optional) uncomment the below line to save the UMAP image patch projection\n",
    "# fig.savefig(f\"umap_{mapper.n_neighbors}nbs_rnd{mapper.random_state}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
